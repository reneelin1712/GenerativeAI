{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6089,"status":"ok","timestamp":1681447550780,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"dOQC4Z496PUX"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\"\"\"\n","Architecture based on InfoGAN paper.\n","\"\"\"\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.tconv1 = nn.ConvTranspose2d(74, 1024, 1, 1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(1024)\n","\n","        self.tconv2 = nn.ConvTranspose2d(1024, 128, 7, 1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(128)\n","\n","        self.tconv3 = nn.ConvTranspose2d(128, 64, 4, 2, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(64)\n","\n","        self.tconv4 = nn.ConvTranspose2d(64, 1, 4, 2, padding=1, bias=False)\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.tconv1(x)))\n","        x = F.relu(self.bn2(self.tconv2(x)))\n","        x = F.relu(self.bn3(self.tconv3(x)))\n","\n","        img = torch.sigmoid(self.tconv4(x))\n","\n","        return img\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 64, 4, 2, 1)\n","\n","        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(128)\n","\n","        self.conv3 = nn.Conv2d(128, 1024, 7, bias=False)\n","        self.bn3 = nn.BatchNorm2d(1024)\n","\n","    def forward(self, x):\n","        x = F.leaky_relu(self.conv1(x), 0.1, inplace=True)\n","        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.1, inplace=True)\n","        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.1, inplace=True)\n","\n","        return x"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681447550780,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"txiVT33q6YxJ"},"outputs":[],"source":["class DHead(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv = nn.Conv2d(1024, 1, 1)\n","\n","    def forward(self, x):\n","        output = torch.sigmoid(self.conv(x))\n","\n","        return output"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681447550781,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"5wpilYap6jNw"},"outputs":[],"source":["class QHead(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(1024, 128, 1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(128)\n","\n","        self.conv_disc = nn.Conv2d(128, 10, 1)\n","        self.conv_mu = nn.Conv2d(128, 2, 1)\n","        self.conv_var = nn.Conv2d(128, 2, 1)\n","\n","    def forward(self, x):\n","        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.1, inplace=True)\n","\n","        disc_logits = self.conv_disc(x).squeeze()\n","\n","        mu = self.conv_mu(x).squeeze()\n","        var = torch.exp(self.conv_var(x).squeeze())\n","\n","        return disc_logits, mu, var"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681447550781,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"Jemvgc7L7w6s"},"outputs":[],"source":["params = {\n","    'batch_size': 128,# Batch size.\n","    'num_epochs': 100,# Number of epochs to train for.\n","    'learning_rate': 2e-4,# Learning rate.\n","    'beta1': 0.5,\n","    'beta2': 0.999,\n","    'save_epoch' : 25,# After how many epochs to save checkpoints and generate test output.\n","    'dataset' : 'MNIST',\n","    \n","    \n","    }\n","\n","params['num_z'] = 62\n","params['num_dis_c'] = 1\n","params['dis_c_dim'] = 10\n","params['num_con_c'] = 2\n","\n","batch_size = params['batch_size']"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":529,"status":"ok","timestamp":1681447551306,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"LMevXdSr6vWu"},"outputs":[],"source":["import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1576,"status":"ok","timestamp":1681447552876,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"CwMj8MB76vZu","outputId":"e50f333a-3c86-4eca-d7e6-f76538d7af5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00\u003c00:00, 71132334.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/mnist/MNIST/raw/train-images-idx3-ubyte.gz to dataset/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00\u003c00:00, 113849336.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00\u003c00:00, 26573213.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00\u003c00:00, 21747178.96it/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting dataset/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/mnist/MNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["transform = transforms.Compose([\n","            transforms.Resize(28),\n","            transforms.CenterCrop(28),\n","            transforms.ToTensor()])\n","\n","# transforms = transforms.Compose(\n","#     [\n","#         transforms.ToTensor(),\n","#         transforms.Normalize((0.5,), (0.5,)),\n","#     ]\n","# )\n","\n","# dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n","dataset = datasets.MNIST(\"dataset/\"+'mnist/', train='train', \n","                                download=True, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","                                      "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1681447552876,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"H9HliT2I8MNA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1681447552877,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"uE-qNtL07CFC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import time\n","import random"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1681447552877,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"8wMWCFxk7JVG","outputId":"04ff71b3-c986-4ae5-9868-8214b62ef0c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed:  1123\n","cuda:0  will be used.\n","\n"]}],"source":["# Set random seed for reproducibility.\n","seed = 1123\n","random.seed(seed)\n","torch.manual_seed(seed)\n","print(\"Random Seed: \", seed)\n","\n","device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n","print(device, \" will be used.\\n\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9979,"status":"ok","timestamp":1681447562854,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"tQInOp4w8M01"},"outputs":[],"source":["# Initialise the network.\n","netG = Generator().to(device)\n","# netG.apply(weights_init)\n","# print(netG)\n","\n","discriminator = Discriminator().to(device)\n","# discriminator.apply(weights_init)\n","# print(discriminator)\n","\n","netD = DHead().to(device)\n","# netD.apply(weights_init)\n","# print(netD)\n","\n","netQ = QHead().to(device)\n","# netQ.apply(weights_init)\n","# print(netQ)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1681447562854,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"VSK9EkSX-iQw"},"outputs":[],"source":["class NormalNLLLoss:\n","    \"\"\"\n","    Calculate the negative log likelihood\n","    of normal distribution.\n","    This needs to be minimised.\n","    Treating Q(cj | x) as a factored Gaussian.\n","    \"\"\"\n","    def __call__(self, x, mu, var):\n","        \n","        logli = -0.5 * (var.mul(2 * np.pi) + 1e-6).log() - (x - mu).pow(2).div(var.mul(2.0) + 1e-6)\n","        nll = -(logli.sum(1).mean())\n","\n","        return nll\n","\n","def noise_sample(n_dis_c, dis_c_dim, n_con_c, n_z, batch_size, device):\n","    \"\"\"\n","    Sample random noise vector for training.\n","    INPUT\n","    --------\n","    n_dis_c : Number of discrete latent code.\n","    dis_c_dim : Dimension of discrete latent code.\n","    n_con_c : Number of continuous latent code.\n","    n_z : Dimension of iicompressible noise.\n","    batch_size : Batch Size\n","    device : GPU/CPU\n","    \"\"\"\n","\n","    z = torch.randn(batch_size, n_z, 1, 1, device=device)\n","\n","    idx = np.zeros((n_dis_c, batch_size))\n","    if(n_dis_c != 0):\n","        dis_c = torch.zeros(batch_size, n_dis_c, dis_c_dim, device=device)\n","        \n","        for i in range(n_dis_c):\n","            idx[i] = np.random.randint(dis_c_dim, size=batch_size)\n","            dis_c[torch.arange(0, batch_size), i, idx[i]] = 1.0\n","\n","        dis_c = dis_c.view(batch_size, -1, 1, 1)\n","\n","    if(n_con_c != 0):\n","        # Random uniform between -1 and 1.\n","        con_c = torch.rand(batch_size, n_con_c, 1, 1, device=device) * 2 - 1\n","\n","    noise = z\n","    if(n_dis_c != 0):\n","        noise = torch.cat((z, dis_c), dim=1)\n","    if(n_con_c != 0):\n","        noise = torch.cat((noise, con_c), dim=1)\n","\n","    return noise, idx"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1681447562855,"user":{"displayName":"Renee Lin","userId":"04465224553407414346"},"user_tz":-600},"id":"qYa2y6PT8Vga","outputId":"806d8de1-2d53-45a7-d7bd-97653e390b30"},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------\n","Starting Training Loop...\n","\n","Epochs: 100\n","Dataset: MNIST\n","Batch Size: 128\n","Length of Data Loader: 469\n","-------------------------\n"]}],"source":["# Loss for discrimination between real and fake images.\n","criterionD = nn.BCELoss()\n","# Loss for discrete latent code.\n","criterionQ_dis = nn.CrossEntropyLoss()\n","# Loss for continuous latent code.\n","criterionQ_con = NormalNLLLoss()\n","\n","# Adam optimiser is used.\n","optimD = optim.Adam([{'params': discriminator.parameters()}, {'params': netD.parameters()}], lr=params['learning_rate'], betas=(params['beta1'], params['beta2']))\n","optimG = optim.Adam([{'params': netG.parameters()}, {'params': netQ.parameters()}], lr=params['learning_rate'], betas=(params['beta1'], params['beta2']))\n","\n","# Fixed Noise\n","z = torch.randn(100, params['num_z'], 1, 1, device=device)\n","fixed_noise = z\n","if(params['num_dis_c'] != 0):\n","    idx = np.arange(params['dis_c_dim']).repeat(10)\n","    dis_c = torch.zeros(100, params['num_dis_c'], params['dis_c_dim'], device=device)\n","    for i in range(params['num_dis_c']):\n","        dis_c[torch.arange(0, 100), i, idx] = 1.0\n","\n","    dis_c = dis_c.view(100, -1, 1, 1)\n","\n","    fixed_noise = torch.cat((fixed_noise, dis_c), dim=1)\n","\n","if(params['num_con_c'] != 0):\n","    con_c = torch.rand(100, params['num_con_c'], 1, 1, device=device) * 2 - 1\n","    fixed_noise = torch.cat((fixed_noise, con_c), dim=1)\n","\n","real_label = 1\n","fake_label = 0\n","\n","# List variables to store results pf training.\n","img_list = []\n","G_losses = []\n","D_losses = []\n","\n","print(\"-\"*25)\n","print(\"Starting Training Loop...\\n\")\n","print('Epochs: %d\\nDataset: {}\\nBatch Size: %d\\nLength of Data Loader: %d'.format(params['dataset']) % (params['num_epochs'], params['batch_size'], len(dataloader)))\n","print(\"-\"*25)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"-G-sPyf68h6o"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1/100][100/469]\tLoss_D: 0.7421\tLoss_G: 1.8111\n","[1/100][200/469]\tLoss_D: 0.6130\tLoss_G: 2.0555\n","[1/100][300/469]\tLoss_D: 0.5301\tLoss_G: 2.0982\n","[1/100][400/469]\tLoss_D: 0.6453\tLoss_G: 1.7104\n","Time taken for Epoch 1: 110.98s\n","[2/100][100/469]\tLoss_D: 0.6992\tLoss_G: 1.8083\n","[2/100][200/469]\tLoss_D: 0.7628\tLoss_G: 1.6300\n","[2/100][300/469]\tLoss_D: 0.9428\tLoss_G: 1.6084\n","[2/100][400/469]\tLoss_D: 0.7512\tLoss_G: 1.6174\n","Time taken for Epoch 2: 102.73s\n","[3/100][100/469]\tLoss_D: 0.8533\tLoss_G: 1.5077\n","[3/100][200/469]\tLoss_D: 0.9004\tLoss_G: 1.6997\n","[3/100][300/469]\tLoss_D: 0.9081\tLoss_G: 1.7361\n","[3/100][400/469]\tLoss_D: 0.8434\tLoss_G: 1.4637\n","Time taken for Epoch 3: 102.64s\n","[4/100][100/469]\tLoss_D: 0.7802\tLoss_G: 1.4569\n","[4/100][200/469]\tLoss_D: 1.2223\tLoss_G: 1.6648\n","[4/100][300/469]\tLoss_D: 0.8861\tLoss_G: 1.3952\n","[4/100][400/469]\tLoss_D: 0.8892\tLoss_G: 1.2525\n","Time taken for Epoch 4: 102.97s\n","[5/100][100/469]\tLoss_D: 0.8327\tLoss_G: 1.4636\n","[5/100][200/469]\tLoss_D: 0.9174\tLoss_G: 1.5478\n","[5/100][300/469]\tLoss_D: 0.8222\tLoss_G: 1.3054\n","[5/100][400/469]\tLoss_D: 1.0516\tLoss_G: 1.3064\n","Time taken for Epoch 5: 102.85s\n","[6/100][100/469]\tLoss_D: 0.8258\tLoss_G: 1.3115\n","[6/100][200/469]\tLoss_D: 0.9771\tLoss_G: 1.6148\n","[6/100][300/469]\tLoss_D: 0.8829\tLoss_G: 1.5105\n","[6/100][400/469]\tLoss_D: 1.0227\tLoss_G: 1.6035\n","Time taken for Epoch 6: 102.74s\n","[7/100][100/469]\tLoss_D: 0.9455\tLoss_G: 1.3444\n","[7/100][200/469]\tLoss_D: 0.8735\tLoss_G: 1.4205\n","[7/100][300/469]\tLoss_D: 0.9358\tLoss_G: 1.2885\n","[7/100][400/469]\tLoss_D: 0.8551\tLoss_G: 1.2575\n","Time taken for Epoch 7: 102.74s\n","[8/100][100/469]\tLoss_D: 0.9831\tLoss_G: 1.5341\n","[8/100][200/469]\tLoss_D: 0.7562\tLoss_G: 1.4908\n","[8/100][300/469]\tLoss_D: 0.8554\tLoss_G: 1.7345\n","[8/100][400/469]\tLoss_D: 0.7411\tLoss_G: 1.4118\n","Time taken for Epoch 8: 102.88s\n","[9/100][100/469]\tLoss_D: 0.8232\tLoss_G: 1.6954\n","[9/100][200/469]\tLoss_D: 1.0116\tLoss_G: 1.6577\n","[9/100][300/469]\tLoss_D: 0.9371\tLoss_G: 1.3484\n","[9/100][400/469]\tLoss_D: 0.9566\tLoss_G: 1.5957\n","Time taken for Epoch 9: 102.68s\n","[10/100][100/469]\tLoss_D: 0.7614\tLoss_G: 1.7596\n","[10/100][200/469]\tLoss_D: 0.9057\tLoss_G: 1.3426\n","[10/100][300/469]\tLoss_D: 0.7686\tLoss_G: 1.7079\n","[10/100][400/469]\tLoss_D: 0.8389\tLoss_G: 1.7844\n","Time taken for Epoch 10: 102.82s\n","[11/100][100/469]\tLoss_D: 0.6924\tLoss_G: 1.8310\n","[11/100][200/469]\tLoss_D: 0.6879\tLoss_G: 1.8008\n","[11/100][300/469]\tLoss_D: 0.8594\tLoss_G: 1.7752\n","[11/100][400/469]\tLoss_D: 0.7643\tLoss_G: 2.0656\n","Time taken for Epoch 11: 102.95s\n","[12/100][100/469]\tLoss_D: 0.7643\tLoss_G: 1.6229\n","[12/100][200/469]\tLoss_D: 0.7806\tLoss_G: 1.6732\n","[12/100][300/469]\tLoss_D: 0.8236\tLoss_G: 1.5168\n","[12/100][400/469]\tLoss_D: 0.7177\tLoss_G: 1.4044\n","Time taken for Epoch 12: 103.07s\n","[13/100][100/469]\tLoss_D: 0.8026\tLoss_G: 2.0267\n","[13/100][200/469]\tLoss_D: 0.6610\tLoss_G: 1.8888\n","[13/100][300/469]\tLoss_D: 0.7239\tLoss_G: 1.5503\n","[13/100][400/469]\tLoss_D: 0.9043\tLoss_G: 1.9172\n","Time taken for Epoch 13: 103.03s\n","[14/100][100/469]\tLoss_D: 0.5139\tLoss_G: 2.1119\n","[14/100][200/469]\tLoss_D: 0.6078\tLoss_G: 1.8882\n","[14/100][300/469]\tLoss_D: 0.6985\tLoss_G: 1.8246\n","[14/100][400/469]\tLoss_D: 0.6808\tLoss_G: 1.4424\n","Time taken for Epoch 14: 102.86s\n","[15/100][100/469]\tLoss_D: 0.7497\tLoss_G: 1.7572\n","[15/100][200/469]\tLoss_D: 0.7270\tLoss_G: 1.8395\n","[15/100][300/469]\tLoss_D: 0.6316\tLoss_G: 2.0705\n","[15/100][400/469]\tLoss_D: 0.8022\tLoss_G: 1.9829\n","Time taken for Epoch 15: 102.86s\n","[16/100][100/469]\tLoss_D: 0.6433\tLoss_G: 1.6992\n","[16/100][200/469]\tLoss_D: 0.7415\tLoss_G: 1.4677\n","[16/100][300/469]\tLoss_D: 0.6311\tLoss_G: 2.1518\n","[16/100][400/469]\tLoss_D: 0.7510\tLoss_G: 1.4913\n","Time taken for Epoch 16: 102.81s\n","[17/100][100/469]\tLoss_D: 0.6843\tLoss_G: 2.0159\n","[17/100][200/469]\tLoss_D: 0.6005\tLoss_G: 2.1139\n","[17/100][300/469]\tLoss_D: 0.7091\tLoss_G: 1.7380\n","[17/100][400/469]\tLoss_D: 0.6595\tLoss_G: 1.7257\n","Time taken for Epoch 17: 102.69s\n","[18/100][100/469]\tLoss_D: 0.6816\tLoss_G: 2.0559\n","[18/100][200/469]\tLoss_D: 0.7866\tLoss_G: 1.8400\n","[18/100][300/469]\tLoss_D: 0.6799\tLoss_G: 2.4980\n","[18/100][400/469]\tLoss_D: 0.5987\tLoss_G: 2.2492\n","Time taken for Epoch 18: 102.65s\n","[19/100][100/469]\tLoss_D: 0.5628\tLoss_G: 2.1328\n","[19/100][200/469]\tLoss_D: 0.7082\tLoss_G: 2.1551\n","[19/100][300/469]\tLoss_D: 0.7118\tLoss_G: 1.9355\n","[19/100][400/469]\tLoss_D: 0.5878\tLoss_G: 1.6974\n","Time taken for Epoch 19: 102.90s\n","[20/100][100/469]\tLoss_D: 0.6757\tLoss_G: 2.4089\n","[20/100][200/469]\tLoss_D: 0.5500\tLoss_G: 2.2193\n","[20/100][300/469]\tLoss_D: 0.6428\tLoss_G: 2.1933\n","[20/100][400/469]\tLoss_D: 0.6069\tLoss_G: 2.2588\n","Time taken for Epoch 20: 102.72s\n","[21/100][100/469]\tLoss_D: 0.5479\tLoss_G: 1.8309\n","[21/100][200/469]\tLoss_D: 0.5639\tLoss_G: 2.2019\n","[21/100][300/469]\tLoss_D: 0.6352\tLoss_G: 2.3041\n","[21/100][400/469]\tLoss_D: 0.6757\tLoss_G: 2.0104\n","Time taken for Epoch 21: 102.50s\n","[22/100][100/469]\tLoss_D: 0.5335\tLoss_G: 1.9746\n","[22/100][200/469]\tLoss_D: 0.7900\tLoss_G: 2.2714\n","[22/100][300/469]\tLoss_D: 0.5635\tLoss_G: 2.8830\n","[22/100][400/469]\tLoss_D: 0.5863\tLoss_G: 2.2754\n","Time taken for Epoch 22: 102.50s\n","[23/100][100/469]\tLoss_D: 0.4025\tLoss_G: 2.5222\n","[23/100][200/469]\tLoss_D: 0.4982\tLoss_G: 2.4577\n","[23/100][300/469]\tLoss_D: 0.6063\tLoss_G: 2.0987\n","[23/100][400/469]\tLoss_D: 0.5505\tLoss_G: 2.7179\n","Time taken for Epoch 23: 102.70s\n","[24/100][100/469]\tLoss_D: 0.5682\tLoss_G: 1.6754\n","[24/100][200/469]\tLoss_D: 0.3741\tLoss_G: 2.4000\n","[24/100][300/469]\tLoss_D: 0.5784\tLoss_G: 2.7239\n","[24/100][400/469]\tLoss_D: 0.5602\tLoss_G: 1.7087\n","Time taken for Epoch 24: 102.48s\n","[25/100][100/469]\tLoss_D: 0.4268\tLoss_G: 2.7063\n","[25/100][200/469]\tLoss_D: 0.4503\tLoss_G: 2.7176\n","[25/100][300/469]\tLoss_D: 0.7949\tLoss_G: 3.0307\n","[25/100][400/469]\tLoss_D: 0.4412\tLoss_G: 2.8288\n","Time taken for Epoch 25: 102.74s\n","[26/100][100/469]\tLoss_D: 0.5705\tLoss_G: 2.2597\n","[26/100][200/469]\tLoss_D: 0.6392\tLoss_G: 3.3245\n","[26/100][300/469]\tLoss_D: 0.4601\tLoss_G: 2.2347\n","[26/100][400/469]\tLoss_D: 0.4829\tLoss_G: 2.4378\n","Time taken for Epoch 26: 102.56s\n","[27/100][100/469]\tLoss_D: 0.4137\tLoss_G: 2.9349\n","[27/100][200/469]\tLoss_D: 0.5739\tLoss_G: 2.7888\n","[27/100][300/469]\tLoss_D: 0.5231\tLoss_G: 2.9620\n","[27/100][400/469]\tLoss_D: 0.6308\tLoss_G: 2.4367\n","Time taken for Epoch 27: 102.57s\n","[28/100][100/469]\tLoss_D: 0.6952\tLoss_G: 2.6415\n","[28/100][200/469]\tLoss_D: 0.4452\tLoss_G: 2.4913\n","[28/100][300/469]\tLoss_D: 0.4772\tLoss_G: 2.0397\n","[28/100][400/469]\tLoss_D: 0.5054\tLoss_G: 2.5158\n","Time taken for Epoch 28: 102.56s\n","[29/100][100/469]\tLoss_D: 0.3936\tLoss_G: 2.5161\n","[29/100][200/469]\tLoss_D: 0.4585\tLoss_G: 2.4023\n","[29/100][300/469]\tLoss_D: 0.3563\tLoss_G: 2.1025\n","[29/100][400/469]\tLoss_D: 0.4350\tLoss_G: 2.8800\n","Time taken for Epoch 29: 102.84s\n","[30/100][100/469]\tLoss_D: 0.4338\tLoss_G: 2.3988\n","[30/100][200/469]\tLoss_D: 0.5424\tLoss_G: 2.8902\n","[30/100][300/469]\tLoss_D: 0.3990\tLoss_G: 2.8794\n","[30/100][400/469]\tLoss_D: 0.5683\tLoss_G: 2.0326\n","Time taken for Epoch 30: 102.63s\n","[31/100][100/469]\tLoss_D: 0.3371\tLoss_G: 3.2658\n","[31/100][200/469]\tLoss_D: 0.5297\tLoss_G: 2.0002\n","[31/100][300/469]\tLoss_D: 0.5068\tLoss_G: 2.6929\n","[31/100][400/469]\tLoss_D: 0.4573\tLoss_G: 2.9689\n","Time taken for Epoch 31: 102.62s\n","[32/100][100/469]\tLoss_D: 0.4333\tLoss_G: 3.3671\n","[32/100][200/469]\tLoss_D: 0.4612\tLoss_G: 2.2965\n","[32/100][300/469]\tLoss_D: 0.5622\tLoss_G: 1.9674\n","[32/100][400/469]\tLoss_D: 0.5143\tLoss_G: 2.2618\n","Time taken for Epoch 32: 102.44s\n","[33/100][100/469]\tLoss_D: 0.3860\tLoss_G: 2.6431\n","[33/100][200/469]\tLoss_D: 0.4611\tLoss_G: 2.5049\n","[33/100][300/469]\tLoss_D: 0.5435\tLoss_G: 2.4975\n","[33/100][400/469]\tLoss_D: 0.3928\tLoss_G: 3.2485\n","Time taken for Epoch 33: 102.24s\n","[34/100][100/469]\tLoss_D: 0.6312\tLoss_G: 1.9125\n","[34/100][200/469]\tLoss_D: 0.4735\tLoss_G: 2.9023\n","[34/100][300/469]\tLoss_D: 0.4876\tLoss_G: 2.4696\n","[34/100][400/469]\tLoss_D: 0.3930\tLoss_G: 2.7681\n","Time taken for Epoch 34: 102.45s\n","[35/100][100/469]\tLoss_D: 0.4432\tLoss_G: 2.8379\n","[35/100][200/469]\tLoss_D: 0.5379\tLoss_G: 2.6509\n","[35/100][300/469]\tLoss_D: 0.3790\tLoss_G: 2.7351\n","[35/100][400/469]\tLoss_D: 0.3683\tLoss_G: 2.0866\n","Time taken for Epoch 35: 102.43s\n","[36/100][100/469]\tLoss_D: 0.4798\tLoss_G: 2.6423\n","[36/100][200/469]\tLoss_D: 0.5701\tLoss_G: 4.3915\n","[36/100][300/469]\tLoss_D: 0.4329\tLoss_G: 3.3949\n","[36/100][400/469]\tLoss_D: 0.4808\tLoss_G: 2.5742\n","Time taken for Epoch 36: 102.39s\n","[37/100][100/469]\tLoss_D: 0.4488\tLoss_G: 2.7748\n","[37/100][200/469]\tLoss_D: 0.4921\tLoss_G: 2.8615\n","[37/100][300/469]\tLoss_D: 0.4920\tLoss_G: 2.8263\n","[37/100][400/469]\tLoss_D: 0.4905\tLoss_G: 2.7737\n","Time taken for Epoch 37: 102.26s\n","[38/100][100/469]\tLoss_D: 0.5035\tLoss_G: 2.2538\n","[38/100][200/469]\tLoss_D: 0.3831\tLoss_G: 2.8184\n","[38/100][300/469]\tLoss_D: 0.2836\tLoss_G: 2.8286\n","[38/100][400/469]\tLoss_D: 0.3575\tLoss_G: 3.1169\n","Time taken for Epoch 38: 102.23s\n","[39/100][100/469]\tLoss_D: 0.2875\tLoss_G: 3.1427\n","[39/100][200/469]\tLoss_D: 0.3017\tLoss_G: 2.7212\n","[39/100][300/469]\tLoss_D: 0.4396\tLoss_G: 3.1435\n","[39/100][400/469]\tLoss_D: 0.4131\tLoss_G: 3.2208\n","Time taken for Epoch 39: 102.25s\n","[40/100][100/469]\tLoss_D: 0.3351\tLoss_G: 3.5025\n","[40/100][200/469]\tLoss_D: 0.2938\tLoss_G: 2.9301\n","[40/100][300/469]\tLoss_D: 0.4004\tLoss_G: 2.8390\n","[40/100][400/469]\tLoss_D: 0.1998\tLoss_G: 3.5957\n","Time taken for Epoch 40: 102.25s\n","[41/100][100/469]\tLoss_D: 0.2967\tLoss_G: 3.3575\n","[41/100][200/469]\tLoss_D: 0.3773\tLoss_G: 2.3672\n","[41/100][300/469]\tLoss_D: 0.3510\tLoss_G: 3.3873\n","[41/100][400/469]\tLoss_D: 0.5932\tLoss_G: 4.0631\n","Time taken for Epoch 41: 102.26s\n","[42/100][100/469]\tLoss_D: 0.3077\tLoss_G: 3.3262\n","[42/100][200/469]\tLoss_D: 0.5182\tLoss_G: 3.2993\n","[42/100][300/469]\tLoss_D: 0.3779\tLoss_G: 2.6814\n","[42/100][400/469]\tLoss_D: 0.3835\tLoss_G: 3.0360\n","Time taken for Epoch 42: 102.31s\n","[43/100][100/469]\tLoss_D: 0.4076\tLoss_G: 3.8810\n","[43/100][200/469]\tLoss_D: 0.4495\tLoss_G: 2.3700\n","[43/100][300/469]\tLoss_D: 0.4351\tLoss_G: 3.6322\n","[43/100][400/469]\tLoss_D: 0.3707\tLoss_G: 2.9063\n","Time taken for Epoch 43: 102.27s\n","[44/100][100/469]\tLoss_D: 0.3527\tLoss_G: 3.8490\n","[44/100][200/469]\tLoss_D: 0.3987\tLoss_G: 3.0482\n","[44/100][300/469]\tLoss_D: 0.3639\tLoss_G: 3.0185\n","[44/100][400/469]\tLoss_D: 0.3535\tLoss_G: 2.4693\n","Time taken for Epoch 44: 102.24s\n","[45/100][100/469]\tLoss_D: 0.2689\tLoss_G: 3.2156\n","[45/100][200/469]\tLoss_D: 0.3802\tLoss_G: 2.3743\n","[45/100][300/469]\tLoss_D: 0.3892\tLoss_G: 2.6568\n","[45/100][400/469]\tLoss_D: 0.2468\tLoss_G: 2.8816\n","Time taken for Epoch 45: 102.55s\n","[46/100][100/469]\tLoss_D: 0.3780\tLoss_G: 3.0456\n","[46/100][200/469]\tLoss_D: 0.3401\tLoss_G: 3.2109\n","[46/100][300/469]\tLoss_D: 0.3341\tLoss_G: 2.8225\n","[46/100][400/469]\tLoss_D: 0.3930\tLoss_G: 2.9684\n","Time taken for Epoch 46: 102.49s\n","[47/100][100/469]\tLoss_D: 0.3771\tLoss_G: 2.7256\n","[47/100][200/469]\tLoss_D: 0.3104\tLoss_G: 3.3273\n","[47/100][300/469]\tLoss_D: 0.3536\tLoss_G: 2.7416\n","[47/100][400/469]\tLoss_D: 0.4441\tLoss_G: 3.1427\n","Time taken for Epoch 47: 102.46s\n","[48/100][100/469]\tLoss_D: 0.3102\tLoss_G: 4.0935\n","[48/100][200/469]\tLoss_D: 0.4490\tLoss_G: 3.4045\n","[48/100][300/469]\tLoss_D: 0.3849\tLoss_G: 2.2405\n","[48/100][400/469]\tLoss_D: 0.3741\tLoss_G: 3.4755\n","Time taken for Epoch 48: 102.50s\n","[49/100][100/469]\tLoss_D: 0.3941\tLoss_G: 2.2890\n","[49/100][200/469]\tLoss_D: 0.3927\tLoss_G: 2.7598\n","[49/100][300/469]\tLoss_D: 0.2767\tLoss_G: 3.4356\n","[49/100][400/469]\tLoss_D: 0.3947\tLoss_G: 3.9753\n","Time taken for Epoch 49: 102.37s\n","[50/100][100/469]\tLoss_D: 0.5258\tLoss_G: 4.0315\n","[50/100][200/469]\tLoss_D: 0.2575\tLoss_G: 3.4610\n","[50/100][300/469]\tLoss_D: 0.3984\tLoss_G: 2.8560\n","[50/100][400/469]\tLoss_D: 0.3429\tLoss_G: 2.8988\n","Time taken for Epoch 50: 102.40s\n","[51/100][100/469]\tLoss_D: 0.2565\tLoss_G: 2.8810\n","[51/100][200/469]\tLoss_D: 0.2997\tLoss_G: 3.7356\n","[51/100][300/469]\tLoss_D: 0.3670\tLoss_G: 2.5192\n","[51/100][400/469]\tLoss_D: 0.2985\tLoss_G: 3.5790\n","Time taken for Epoch 51: 102.30s\n","[52/100][100/469]\tLoss_D: 0.7311\tLoss_G: 2.3364\n","[52/100][200/469]\tLoss_D: 0.4832\tLoss_G: 4.5081\n","[52/100][300/469]\tLoss_D: 0.3202\tLoss_G: 3.2455\n","[52/100][400/469]\tLoss_D: 0.2853\tLoss_G: 2.8166\n","Time taken for Epoch 52: 102.53s\n","[53/100][100/469]\tLoss_D: 0.4466\tLoss_G: 2.1241\n","[53/100][200/469]\tLoss_D: 0.3346\tLoss_G: 3.3427\n","[53/100][300/469]\tLoss_D: 0.5204\tLoss_G: 3.7132\n","[53/100][400/469]\tLoss_D: 0.3305\tLoss_G: 3.6523\n","Time taken for Epoch 53: 102.37s\n","[54/100][100/469]\tLoss_D: 0.3237\tLoss_G: 3.5333\n","[54/100][200/469]\tLoss_D: 0.3518\tLoss_G: 2.6549\n","[54/100][300/469]\tLoss_D: 0.4088\tLoss_G: 4.0816\n","[54/100][400/469]\tLoss_D: 0.2304\tLoss_G: 4.0796\n","Time taken for Epoch 54: 102.48s\n","[55/100][100/469]\tLoss_D: 0.3987\tLoss_G: 4.1649\n","[55/100][200/469]\tLoss_D: 0.2916\tLoss_G: 3.9897\n","[55/100][300/469]\tLoss_D: 0.3626\tLoss_G: 3.3382\n","[55/100][400/469]\tLoss_D: 0.2071\tLoss_G: 4.0000\n","Time taken for Epoch 55: 102.50s\n","[56/100][100/469]\tLoss_D: 0.3056\tLoss_G: 3.4602\n","[56/100][200/469]\tLoss_D: 0.4351\tLoss_G: 3.0486\n","[56/100][300/469]\tLoss_D: 0.3372\tLoss_G: 3.1051\n","[56/100][400/469]\tLoss_D: 0.3008\tLoss_G: 3.7891\n","Time taken for Epoch 56: 102.42s\n","[57/100][100/469]\tLoss_D: 0.2038\tLoss_G: 3.6973\n","[57/100][200/469]\tLoss_D: 0.2367\tLoss_G: 2.9667\n","[57/100][300/469]\tLoss_D: 0.3160\tLoss_G: 3.2662\n","[57/100][400/469]\tLoss_D: 0.3117\tLoss_G: 4.0899\n","Time taken for Epoch 57: 102.45s\n","[58/100][100/469]\tLoss_D: 0.1975\tLoss_G: 3.4735\n","[58/100][200/469]\tLoss_D: 0.3207\tLoss_G: 3.4368\n","[58/100][300/469]\tLoss_D: 0.2857\tLoss_G: 3.2438\n","[58/100][400/469]\tLoss_D: 0.2091\tLoss_G: 4.0756\n","Time taken for Epoch 58: 102.49s\n","[59/100][100/469]\tLoss_D: 0.2585\tLoss_G: 3.3535\n","[59/100][200/469]\tLoss_D: 0.3299\tLoss_G: 2.7728\n","[59/100][300/469]\tLoss_D: 0.2534\tLoss_G: 3.2902\n","[59/100][400/469]\tLoss_D: 0.2812\tLoss_G: 3.7722\n","Time taken for Epoch 59: 102.45s\n","[60/100][100/469]\tLoss_D: 0.3216\tLoss_G: 3.0546\n","[60/100][200/469]\tLoss_D: 0.4248\tLoss_G: 4.4654\n","[60/100][300/469]\tLoss_D: 0.3969\tLoss_G: 3.4635\n","[60/100][400/469]\tLoss_D: 0.2506\tLoss_G: 3.5384\n","Time taken for Epoch 60: 102.27s\n","[61/100][100/469]\tLoss_D: 0.3393\tLoss_G: 2.8782\n","[61/100][200/469]\tLoss_D: 0.2470\tLoss_G: 2.9932\n","[61/100][300/469]\tLoss_D: 0.4436\tLoss_G: 5.0311\n","[61/100][400/469]\tLoss_D: 0.4641\tLoss_G: 2.7874\n","Time taken for Epoch 61: 102.34s\n","[62/100][100/469]\tLoss_D: 0.2893\tLoss_G: 2.7811\n","[62/100][200/469]\tLoss_D: 0.2724\tLoss_G: 3.8840\n","[62/100][300/469]\tLoss_D: 0.2580\tLoss_G: 3.4313\n","[62/100][400/469]\tLoss_D: 0.2415\tLoss_G: 3.9684\n","Time taken for Epoch 62: 102.29s\n","[63/100][100/469]\tLoss_D: 0.3261\tLoss_G: 3.1841\n","[63/100][200/469]\tLoss_D: 0.1896\tLoss_G: 3.7042\n","[63/100][300/469]\tLoss_D: 0.3660\tLoss_G: 3.6631\n","[63/100][400/469]\tLoss_D: 0.3571\tLoss_G: 4.6025\n","Time taken for Epoch 63: 102.36s\n","[64/100][100/469]\tLoss_D: 0.4008\tLoss_G: 2.4161\n","[64/100][200/469]\tLoss_D: 0.1932\tLoss_G: 3.8223\n","[64/100][300/469]\tLoss_D: 0.2416\tLoss_G: 3.4964\n","[64/100][400/469]\tLoss_D: 0.1991\tLoss_G: 3.6689\n","Time taken for Epoch 64: 102.33s\n","[65/100][100/469]\tLoss_D: 0.2285\tLoss_G: 4.0906\n","[65/100][200/469]\tLoss_D: 0.2061\tLoss_G: 3.7244\n","[65/100][300/469]\tLoss_D: 0.2066\tLoss_G: 3.7866\n","[65/100][400/469]\tLoss_D: 0.3579\tLoss_G: 4.8210\n","Time taken for Epoch 65: 102.37s\n","[66/100][100/469]\tLoss_D: 0.2786\tLoss_G: 3.6456\n","[66/100][200/469]\tLoss_D: 0.2953\tLoss_G: 3.8757\n","[66/100][300/469]\tLoss_D: 0.2682\tLoss_G: 3.9202\n","[66/100][400/469]\tLoss_D: 0.2918\tLoss_G: 3.8577\n","Time taken for Epoch 66: 102.47s\n","[67/100][100/469]\tLoss_D: 0.1901\tLoss_G: 3.9681\n","[67/100][200/469]\tLoss_D: 0.3050\tLoss_G: 3.9803\n","[67/100][300/469]\tLoss_D: 0.2613\tLoss_G: 4.1338\n","[67/100][400/469]\tLoss_D: 0.2014\tLoss_G: 3.7725\n","Time taken for Epoch 67: 102.26s\n","[68/100][100/469]\tLoss_D: 0.2638\tLoss_G: 3.4718\n","[68/100][200/469]\tLoss_D: 0.3178\tLoss_G: 4.2923\n","[68/100][300/469]\tLoss_D: 0.2474\tLoss_G: 3.4478\n","[68/100][400/469]\tLoss_D: 0.2156\tLoss_G: 3.5833\n","Time taken for Epoch 68: 102.28s\n","[69/100][100/469]\tLoss_D: 0.3581\tLoss_G: 4.2363\n","[69/100][200/469]\tLoss_D: 0.2410\tLoss_G: 4.4236\n","[69/100][300/469]\tLoss_D: 0.2062\tLoss_G: 3.4750\n","[69/100][400/469]\tLoss_D: 0.2585\tLoss_G: 3.8628\n","Time taken for Epoch 69: 102.25s\n","[70/100][100/469]\tLoss_D: 0.1969\tLoss_G: 4.4525\n","[70/100][200/469]\tLoss_D: 0.2834\tLoss_G: 4.4169\n","[70/100][300/469]\tLoss_D: 0.2525\tLoss_G: 3.9758\n","[70/100][400/469]\tLoss_D: 0.2250\tLoss_G: 3.5426\n","Time taken for Epoch 70: 102.51s\n","[71/100][100/469]\tLoss_D: 0.2255\tLoss_G: 3.5321\n","[71/100][200/469]\tLoss_D: 0.2451\tLoss_G: 3.2948\n","[71/100][300/469]\tLoss_D: 0.2597\tLoss_G: 3.6066\n","[71/100][400/469]\tLoss_D: 0.3971\tLoss_G: 4.0610\n","Time taken for Epoch 71: 102.32s\n","[72/100][100/469]\tLoss_D: 0.4560\tLoss_G: 4.3931\n","[72/100][200/469]\tLoss_D: 0.2224\tLoss_G: 3.3105\n","[72/100][300/469]\tLoss_D: 0.2767\tLoss_G: 3.3324\n","[72/100][400/469]\tLoss_D: 0.5150\tLoss_G: 5.7555\n","Time taken for Epoch 72: 102.37s\n","[73/100][100/469]\tLoss_D: 0.2263\tLoss_G: 4.0700\n","[73/100][200/469]\tLoss_D: 0.2574\tLoss_G: 3.3972\n","[73/100][300/469]\tLoss_D: 0.2070\tLoss_G: 4.3317\n","[73/100][400/469]\tLoss_D: 0.3941\tLoss_G: 4.3978\n","Time taken for Epoch 73: 102.13s\n","[74/100][100/469]\tLoss_D: 0.2907\tLoss_G: 3.0620\n","[74/100][200/469]\tLoss_D: 0.2412\tLoss_G: 3.9532\n","[74/100][300/469]\tLoss_D: 0.3033\tLoss_G: 3.7097\n","[74/100][400/469]\tLoss_D: 0.3271\tLoss_G: 3.3835\n","Time taken for Epoch 74: 102.27s\n","[75/100][100/469]\tLoss_D: 0.2399\tLoss_G: 3.2584\n","[75/100][200/469]\tLoss_D: 0.3217\tLoss_G: 3.4402\n","[75/100][300/469]\tLoss_D: 0.2529\tLoss_G: 3.2291\n","[75/100][400/469]\tLoss_D: 0.2479\tLoss_G: 3.9407\n","Time taken for Epoch 75: 102.14s\n","[76/100][100/469]\tLoss_D: 0.1951\tLoss_G: 4.1459\n","[76/100][200/469]\tLoss_D: 0.1899\tLoss_G: 4.6430\n","[76/100][300/469]\tLoss_D: 0.2095\tLoss_G: 4.5242\n","[76/100][400/469]\tLoss_D: 0.2198\tLoss_G: 3.8992\n","Time taken for Epoch 76: 102.18s\n","[77/100][100/469]\tLoss_D: 0.2825\tLoss_G: 3.3230\n","[77/100][200/469]\tLoss_D: 0.2048\tLoss_G: 3.7347\n","[77/100][300/469]\tLoss_D: 0.3695\tLoss_G: 3.1975\n","[77/100][400/469]\tLoss_D: 0.3815\tLoss_G: 4.2842\n","Time taken for Epoch 77: 102.30s\n","[78/100][100/469]\tLoss_D: 0.2241\tLoss_G: 5.0101\n","[78/100][200/469]\tLoss_D: 0.3105\tLoss_G: 3.4076\n","[78/100][300/469]\tLoss_D: 0.2152\tLoss_G: 4.6528\n","[78/100][400/469]\tLoss_D: 0.3625\tLoss_G: 3.2462\n","Time taken for Epoch 78: 102.09s\n","[79/100][100/469]\tLoss_D: 0.2108\tLoss_G: 3.9135\n","[79/100][200/469]\tLoss_D: 0.3237\tLoss_G: 4.5881\n","[79/100][300/469]\tLoss_D: 0.1243\tLoss_G: 5.1465\n","[79/100][400/469]\tLoss_D: 0.4363\tLoss_G: 3.7546\n","Time taken for Epoch 79: 102.14s\n","[80/100][100/469]\tLoss_D: 0.3853\tLoss_G: 3.1240\n","[80/100][200/469]\tLoss_D: 0.2900\tLoss_G: 4.2455\n","[80/100][300/469]\tLoss_D: 0.2028\tLoss_G: 3.4359\n","[80/100][400/469]\tLoss_D: 0.2587\tLoss_G: 3.9449\n","Time taken for Epoch 80: 102.33s\n","[81/100][100/469]\tLoss_D: 0.1328\tLoss_G: 4.1090\n","[81/100][200/469]\tLoss_D: 0.2670\tLoss_G: 3.2785\n","[81/100][300/469]\tLoss_D: 0.2234\tLoss_G: 3.4947\n","[81/100][400/469]\tLoss_D: 0.2471\tLoss_G: 3.5505\n","Time taken for Epoch 81: 102.23s\n","[82/100][100/469]\tLoss_D: 0.4666\tLoss_G: 4.8032\n","[82/100][200/469]\tLoss_D: 0.3237\tLoss_G: 3.0224\n","[82/100][300/469]\tLoss_D: 0.2748\tLoss_G: 4.0935\n","[82/100][400/469]\tLoss_D: 0.4092\tLoss_G: 4.1485\n","Time taken for Epoch 82: 102.25s\n","[83/100][100/469]\tLoss_D: 0.1927\tLoss_G: 3.9010\n","[83/100][200/469]\tLoss_D: 0.2351\tLoss_G: 4.0600\n","[83/100][300/469]\tLoss_D: 0.4261\tLoss_G: 4.6496\n","[83/100][400/469]\tLoss_D: 0.1754\tLoss_G: 4.8378\n","Time taken for Epoch 83: 102.22s\n","[84/100][100/469]\tLoss_D: 0.1415\tLoss_G: 4.6760\n","[84/100][200/469]\tLoss_D: 0.2983\tLoss_G: 5.1904\n","[84/100][300/469]\tLoss_D: 0.1412\tLoss_G: 4.8913\n","[84/100][400/469]\tLoss_D: 0.2608\tLoss_G: 3.8663\n","Time taken for Epoch 84: 102.29s\n","[85/100][100/469]\tLoss_D: 0.1551\tLoss_G: 4.8900\n","[85/100][200/469]\tLoss_D: 0.2298\tLoss_G: 3.7731\n","[85/100][300/469]\tLoss_D: 0.2863\tLoss_G: 3.4877\n","[85/100][400/469]\tLoss_D: 0.2050\tLoss_G: 3.5331\n","Time taken for Epoch 85: 102.34s\n","[86/100][100/469]\tLoss_D: 0.4234\tLoss_G: 5.8418\n","[86/100][200/469]\tLoss_D: 0.2733\tLoss_G: 4.3280\n","[86/100][300/469]\tLoss_D: 0.4001\tLoss_G: 4.1956\n","[86/100][400/469]\tLoss_D: 0.2712\tLoss_G: 4.7945\n","Time taken for Epoch 86: 102.26s\n","[87/100][100/469]\tLoss_D: 0.2262\tLoss_G: 4.3417\n","[87/100][200/469]\tLoss_D: 0.3266\tLoss_G: 4.5967\n","[87/100][300/469]\tLoss_D: 0.2336\tLoss_G: 4.8899\n","[87/100][400/469]\tLoss_D: 0.1976\tLoss_G: 4.1417\n","Time taken for Epoch 87: 102.28s\n","[88/100][100/469]\tLoss_D: 0.3632\tLoss_G: 3.8859\n","[88/100][200/469]\tLoss_D: 0.3182\tLoss_G: 3.3436\n","[88/100][300/469]\tLoss_D: 0.3936\tLoss_G: 4.9830\n","[88/100][400/469]\tLoss_D: 0.2072\tLoss_G: 4.0700\n","Time taken for Epoch 88: 102.40s\n","[89/100][100/469]\tLoss_D: 0.3733\tLoss_G: 5.3059\n","[89/100][200/469]\tLoss_D: 0.1676\tLoss_G: 3.9592\n","[89/100][300/469]\tLoss_D: 0.1392\tLoss_G: 4.3726\n","[89/100][400/469]\tLoss_D: 0.2212\tLoss_G: 3.8692\n","Time taken for Epoch 89: 102.34s\n","[90/100][100/469]\tLoss_D: 0.2481\tLoss_G: 4.2637\n","[90/100][200/469]\tLoss_D: 0.1724\tLoss_G: 4.5741\n","[90/100][300/469]\tLoss_D: 0.2015\tLoss_G: 3.9030\n","[90/100][400/469]\tLoss_D: 0.3193\tLoss_G: 3.8494\n","Time taken for Epoch 90: 102.25s\n","[91/100][100/469]\tLoss_D: 0.2094\tLoss_G: 4.8754\n","[91/100][200/469]\tLoss_D: 0.3196\tLoss_G: 3.9032\n","[91/100][300/469]\tLoss_D: 0.2539\tLoss_G: 4.6205\n","[91/100][400/469]\tLoss_D: 0.3310\tLoss_G: 5.2994\n","Time taken for Epoch 91: 102.32s\n","[92/100][100/469]\tLoss_D: 0.2443\tLoss_G: 4.6824\n","[92/100][200/469]\tLoss_D: 0.2031\tLoss_G: 4.5044\n","[92/100][300/469]\tLoss_D: 0.1593\tLoss_G: 4.4794\n","[92/100][400/469]\tLoss_D: 0.2321\tLoss_G: 4.9723\n","Time taken for Epoch 92: 102.37s\n","[93/100][100/469]\tLoss_D: 0.3321\tLoss_G: 4.1714\n","[93/100][200/469]\tLoss_D: 0.1808\tLoss_G: 4.6679\n","[93/100][300/469]\tLoss_D: 0.2372\tLoss_G: 3.5270\n","[93/100][400/469]\tLoss_D: 0.5029\tLoss_G: 2.8655\n","Time taken for Epoch 93: 102.34s\n","[94/100][100/469]\tLoss_D: 0.2200\tLoss_G: 4.9789\n","[94/100][200/469]\tLoss_D: 0.2935\tLoss_G: 4.3329\n","[94/100][300/469]\tLoss_D: 0.1980\tLoss_G: 4.8146\n","[94/100][400/469]\tLoss_D: 0.2668\tLoss_G: 4.4739\n","Time taken for Epoch 94: 102.47s\n","[95/100][100/469]\tLoss_D: 0.1289\tLoss_G: 4.9003\n","[95/100][200/469]\tLoss_D: 0.2450\tLoss_G: 4.7658\n","[95/100][300/469]\tLoss_D: 0.2642\tLoss_G: 4.5930\n","[95/100][400/469]\tLoss_D: 0.2460\tLoss_G: 4.1413\n","Time taken for Epoch 95: 102.57s\n","[96/100][100/469]\tLoss_D: 0.1738\tLoss_G: 4.5105\n","[96/100][200/469]\tLoss_D: 0.1674\tLoss_G: 4.5434\n","[96/100][300/469]\tLoss_D: 0.2903\tLoss_G: 4.9397\n","[96/100][400/469]\tLoss_D: 0.2132\tLoss_G: 4.3592\n","Time taken for Epoch 96: 102.53s\n","[97/100][100/469]\tLoss_D: 0.2181\tLoss_G: 4.9944\n","[97/100][200/469]\tLoss_D: 0.2632\tLoss_G: 5.0076\n","[97/100][300/469]\tLoss_D: 0.1954\tLoss_G: 4.5114\n","[97/100][400/469]\tLoss_D: 0.1649\tLoss_G: 4.7052\n","Time taken for Epoch 97: 102.43s\n","[98/100][100/469]\tLoss_D: 0.1585\tLoss_G: 4.5388\n","[98/100][200/469]\tLoss_D: 0.2984\tLoss_G: 3.6035\n","[98/100][300/469]\tLoss_D: 0.1972\tLoss_G: 4.3124\n","[98/100][400/469]\tLoss_D: 0.2458\tLoss_G: 4.0616\n","Time taken for Epoch 98: 102.40s\n","[99/100][100/469]\tLoss_D: 0.1830\tLoss_G: 4.8707\n","[99/100][200/469]\tLoss_D: 0.1849\tLoss_G: 4.4787\n","[99/100][300/469]\tLoss_D: 0.2255\tLoss_G: 3.8550\n","[99/100][400/469]\tLoss_D: 0.1547\tLoss_G: 4.1119\n","Time taken for Epoch 99: 102.50s\n","[100/100][100/469]\tLoss_D: 0.1690\tLoss_G: 3.5838\n","[100/100][200/469]\tLoss_D: 0.1318\tLoss_G: 4.2283\n","[100/100][300/469]\tLoss_D: 0.3019\tLoss_G: 5.0469\n","[100/100][400/469]\tLoss_D: 0.2159\tLoss_G: 4.9115\n","Time taken for Epoch 100: 102.30s\n"]}],"source":["start_time = time.time()\n","iters = 0\n","\n","for epoch in range(params['num_epochs']):\n","    epoch_start_time = time.time()\n","\n","    for i, (data, _) in enumerate(dataloader):\n","        # Get batch size\n","        b_size = data.size(0)\n","        # Transfer data tensor to GPU/CPU (device)\n","        real_data = data.to(device)\n","\n","        # Updating discriminator and DHead\n","        optimD.zero_grad()\n","        # Real data\n","        label = torch.full((b_size, ), real_label, device=device)\n","        output1 = discriminator(real_data)\n","        probs_real = netD(output1).view(-1)\n","        loss_real = criterionD(probs_real, label.float())\n","        # Calculate gradients.\n","        loss_real.backward()\n","\n","        # Fake data\n","        label.fill_(fake_label)\n","        noise, idx = noise_sample(params['num_dis_c'], params['dis_c_dim'], params['num_con_c'], params['num_z'], b_size, device)\n","        fake_data = netG(noise)\n","        output2 = discriminator(fake_data.detach())\n","        probs_fake = netD(output2).view(-1)\n","        loss_fake = criterionD(probs_fake, label.float())\n","        # Calculate gradients.\n","        loss_fake.backward()\n","\n","        # Net Loss for the discriminator\n","        D_loss = loss_real + loss_fake\n","        # Update parameters\n","        optimD.step()\n","\n","        # Updating Generator and QHead\n","        optimG.zero_grad()\n","\n","        # Fake data treated as real.\n","        output = discriminator(fake_data)\n","        label.fill_(real_label)\n","        probs_fake = netD(output).view(-1)\n","        gen_loss = criterionD(probs_fake, label.float())\n","\n","        q_logits, q_mu, q_var = netQ(output)\n","        target = torch.LongTensor(idx).to(device)\n","        # Calculating loss for discrete latent code.\n","        dis_loss = 0\n","        for j in range(params['num_dis_c']):\n","            dis_loss += criterionQ_dis(q_logits[:, j*10 : j*10 + 10], target[j])\n","\n","        # Calculating loss for continuous latent code.\n","        con_loss = 0\n","        if (params['num_con_c'] != 0):\n","            con_loss = criterionQ_con(noise[:, params['num_z']+ params['num_dis_c']*params['dis_c_dim'] : ].view(-1, params['num_con_c']), q_mu, q_var)*0.1\n","\n","        # Net loss for generator.\n","        G_loss = gen_loss + dis_loss + con_loss\n","        # Calculate gradients.\n","        G_loss.backward()\n","        # Update parameters.\n","        optimG.step()\n","\n","        # Check progress of training.\n","        if i != 0 and i%100 == 0:\n","            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n","                  % (epoch+1, params['num_epochs'], i, len(dataloader), \n","                    D_loss.item(), G_loss.item()))\n","\n","        # Save the losses for plotting.\n","        G_losses.append(G_loss.item())\n","        D_losses.append(D_loss.item())\n","\n","        iters += 1\n","\n","    epoch_time = time.time() - epoch_start_time\n","    print(\"Time taken for Epoch %d: %.2fs\" %(epoch + 1, epoch_time))\n","    # Generate image after each epoch to check performance of the generator. Used for creating animated gif later.\n","    with torch.no_grad():\n","        gen_data = netG(fixed_noise).detach().cpu()\n","    img_list.append(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kCHWK9ZS8pNW"},"outputs":[{"ename":"IndentationError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-13-e2f45742dc63\u003e\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    if((epoch+1) == 1 or (epoch+1) == params['num_epochs']/2):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"source":[" # Generate image to check performance of generator.\n","    if((epoch+1) == 1 or (epoch+1) == params['num_epochs']/2):\n","        with torch.no_grad():\n","            gen_data = netG(fixed_noise).detach().cpu()\n","        plt.figure(figsize=(10, 10))\n","        plt.axis(\"off\")\n","        plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n","        plt.savefig(\"Epoch_%d {}\".format(params['dataset']) %(epoch+1))\n","        plt.close('all')\n","\n","    # Save network weights.\n","    if (epoch+1) % params['save_epoch'] == 0:\n","        torch.save({\n","            'netG' : netG.state_dict(),\n","            'discriminator' : discriminator.state_dict(),\n","            'netD' : netD.state_dict(),\n","            'netQ' : netQ.state_dict(),\n","            'optimD' : optimD.state_dict(),\n","            'optimG' : optimG.state_dict(),\n","            'params' : params\n","            }, 'checkpoint/model_epoch_%d_{}'.format(params['dataset']) %(epoch+1))\n","\n","training_time = time.time() - start_time\n","print(\"-\"*50)\n","print('Training finished!\\nTotal Time for Training: %.2fm' %(training_time / 60))\n","print(\"-\"*50)\n","\n","# Generate image to check performance of trained generator.\n","with torch.no_grad():\n","    gen_data = netG(fixed_noise).detach().cpu()\n","plt.figure(figsize=(10, 10))\n","plt.axis(\"off\")\n","plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n","plt.savefig(\"Epoch_%d_{}\".format(params['dataset']) %(params['num_epochs']))\n","\n","# Save network weights.\n","torch.save({\n","    'netG' : netG.state_dict(),\n","    'discriminator' : discriminator.state_dict(),\n","    'netD' : netD.state_dict(),\n","    'netQ' : netQ.state_dict(),\n","    'optimD' : optimD.state_dict(),\n","    'optimG' : optimG.state_dict(),\n","    'params' : params\n","    }, 'checkpoint/model_final_{}'.format(params['dataset']))\n","\n","\n","# Plot the training losses.\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses,label=\"G\")\n","plt.plot(D_losses,label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.savefig(\"Loss Curve {}\".format(params['dataset']))\n","\n","# Animation showing the improvements of the generator.\n","fig = plt.figure(figsize=(10,10))\n","plt.axis(\"off\")\n","ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n","anim = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","anim.save('infoGAN_{}.gif'.format(params['dataset']), dpi=80, writer='imagemagick')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP7qTT/epUNHupUzF/Joy8p","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}